{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Methods\n",
    "Generating asv_biomarker and person_biomarker matrices, and biomarker names for SBB1s, ASVs, OTUs, Taxa, and Micropheno biomarkers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append('/oak/stanford/groups/dpwall/computeEnvironments/micropheno/MicroPheno/')\n",
    "#from make_representations.representation_maker import Metagenomic16SRepresentation, FastaRepresentations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import scipy.sparse\n",
    "from scipy.spatial.distance import jaccard\n",
    "from tqdm import tqdm\n",
    "BIOMARKER_DIR = '/home/groups/dpwall/briannac/sequence_based_biomarkers/results/generate_biomarkers/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/software/user/open/py-scikit-learn/0.19.1_py36/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/oak/stanford/groups/dpwall/computeEnvironments/ditaxa/DiTaxa/')\n",
    "from main.DiTaxa import DiTaxaWorkflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_unique_columns(M):\n",
    "    M = M.tocsc()\n",
    "    m, n = M.shape\n",
    "    if not M.has_sorted_indices:\n",
    "        M.sort_indices()\n",
    "    if not M.has_canonical_format:\n",
    "        M.sum_duplicates()\n",
    "    sizes = np.diff(M.indptr)\n",
    "    idx = np.argsort(sizes)\n",
    "    Ms = M@scipy.sparse.csc_matrix((np.ones((n,)), idx, np.arange(n+1)), (n, n))\n",
    "    ssizes = np.diff(Ms.indptr)\n",
    "    ssizes[1:] -= ssizes[:-1]\n",
    "    grpidx, = np.where(ssizes)\n",
    "    grpidx = np.concatenate([grpidx, [n]])\n",
    "    if ssizes[0] == 0:\n",
    "        counts = [np.array([0, grpidx[0]])]\n",
    "    else:\n",
    "        counts = [np.zeros((1,), int)]\n",
    "    ssizes = ssizes[grpidx[:-1]].cumsum()\n",
    "    for i, ss in tqdm(enumerate(ssizes)):\n",
    "        gil, gir = grpidx[i:i+2]\n",
    "        pl, pr = Ms.indptr[[gil, gir]]\n",
    "        dv = Ms.data[pl:pr].view(f'V{ss*Ms.data.dtype.itemsize}')\n",
    "        iv = Ms.indices[pl:pr].view(f'V{ss*Ms.indices.dtype.itemsize}')\n",
    "        idxi = np.lexsort((dv, iv))\n",
    "        dv = dv[idxi]\n",
    "        iv = iv[idxi]\n",
    "        chng, = np.where(np.concatenate(\n",
    "            [[True], (dv[1:] != dv[:-1]) | (iv[1:] != iv[:-1]), [True]]))\n",
    "        counts.append(np.diff(chng))\n",
    "        idx[gil:gir] = idx[gil:gir][idxi]\n",
    "    counts = np.concatenate(counts)\n",
    "    nu = counts.size - 1\n",
    "    uniques = M@scipy.sparse.csc_matrix((np.ones((nu,)), idx[counts[:-1].cumsum()],\n",
    "                                   np.arange(nu + 1)), (n, nu))\n",
    "    return uniques, idx[counts[:-1].cumsum()], counts[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute person x biomarker matrix\n",
    "def SaveMatrices(asv_biomarker, biomarkers, dataset, biomarker_type):\n",
    "    sample_vs_asv = pd.read_table('/home/groups/dpwall/briannac/sequence_based_biomarkers/data/%s/sample_vs_asv.tsv' % dataset.replace('testing_', '').replace('small_', ''), index_col=0)\n",
    "    person_biomarker = scipy.sparse.csc_matrix(sample_vs_asv.transpose())*asv_biomarker\n",
    "    biomarker_exists  = [True for b in biomarkers]\n",
    "        \n",
    "    _, no_ld_idx, _ = sparse_unique_columns(asv_biomarker)\n",
    "    no_ld_idx = sorted(no_ld_idx)\n",
    "\n",
    "    print(\"Number of biomarkers after LD filtering: \", len(no_ld_idx))\n",
    "\n",
    "    #np.save('%sperson_variant%d_condensed' %  (output_dir,order), person_biomarker)\n",
    "    print('Computing biomarker names...')\n",
    "\n",
    "    #asv_biomarker = scipy.sparse.csr_matrix(asv_biomarker[:,no_ld_idx])\n",
    "    person_biomarker = scipy.sparse.csr_matrix(person_biomarker[:,no_ld_idx])\n",
    "    biomarkers = [biomarkers[i] for i in no_ld_idx]\n",
    "    is_abundant = ((person_biomarker>0).mean(axis=0)>.1).tolist()[0]\n",
    "    print(sum(is_abundant), ' biomarkers > 10% freq')\n",
    "    asv_biomarker = asv_biomarker[:,np.where(is_abundant)[0]]\n",
    "    person_biomarker = person_biomarker[:,np.where(is_abundant)[0]]\n",
    "    biomarkers = [biomarkers[i] for i in np.where(is_abundant)[0]]    \n",
    "    unique_idx_current = 0\n",
    "    current_idx = 0\n",
    "    with open(BIOMARKER_DIR + 'biomarker_names_%s_%s.txt' % (biomarker_type, dataset), 'w') as f:\n",
    "        for i,biomarker in tqdm(enumerate(biomarkers)):\n",
    "            #if biomarker_exists[i]:  # If biomarker doesn't even exist, just skip.\n",
    "            #    if unique_idx_current==len(no_ld_idx):\n",
    "            #        break\n",
    "            #    elif current_idx==no_ld_idx[unique_idx_current]:\n",
    "            #        if is_abundant[unique_idx_current]: \n",
    "            f.write(str(biomarker) + '\\n')\n",
    "            #        unique_idx_current += 1\n",
    "            #    current_idx += 1\n",
    "    print(\"Writing to files...\")\n",
    "    scipy.sparse.save_npz(BIOMARKER_DIR + 'asv_vs_biomarker_%s_%s.npz' % (biomarker_type, dataset), asv_biomarker)  # Save sparse matrix of ASV x biomarker.\n",
    "    scipy.sparse.save_npz(BIOMARKER_DIR + 'sample_vs_biomarker_%s_%s.npz' % (biomarker_type, dataset), person_biomarker)  # Save sparse matrix of ASV x biomarker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiTaxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing  obesity\n",
      "1  fasta files found in /scratch/users/briannac/tmp/ditaxa/obesity\n",
      "\t✔ DiTaxa workflow is getting started\n",
      "\t✔ Segmentation inference started.. \n",
      "Segmentation training\n",
      "1 fasta files found in /scratch/users/briannac/tmp/ditaxa/obesity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t✔ Corpus size for training NPE is  5000\n",
      "\t✔ The segmentation training took  31.656883273040876  ms.\n",
      "\t✔ Creating NPE representations ...\n",
      "1 fasta files found in /scratch/users/briannac/tmp/ditaxa/obesity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t✔ Generating the NPE representations at npe_representation/obesity_uniquepiece_-1  2.445842742919922 seconds , using 20cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2218it [00:00, 25494.21it/s]\n",
      "15038it [00:00, 893221.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  30109\n",
      "Computing biomarker names...\n",
      "15038  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing  autism\n",
      "1  fasta files found in /scratch/users/briannac/tmp/ditaxa/autism\n",
      "\t✔ DiTaxa workflow is getting started\n",
      "\t✔ Segmentation inference started.. \n",
      "Segmentation training\n",
      "1 fasta files found in /scratch/users/briannac/tmp/ditaxa/autism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t✔ Corpus size for training NPE is  5000\n",
      "\t✔ The segmentation training took  49.07554459897801  ms.\n",
      "\t✔ Creating NPE representations ...\n",
      "1 fasta files found in /scratch/users/briannac/tmp/ditaxa/autism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t✔ Generating the NPE representations at npe_representation/autism_uniquepiece_-1  1.5521552562713623 seconds , using 20cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1168it [00:00, 23844.84it/s]\n",
      "11880it [00:00, 893990.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  21234\n",
      "Computing biomarker names...\n",
      "11880  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['obesity', 'autism']:\n",
    "    print('processing ', dataset)\n",
    "    P = DiTaxaWorkflow('/scratch/users/briannac/tmp/ditaxa/' + dataset,\n",
    "                       'fa','/scratch/users/briannac/tmp/ditaxa/' + dataset,dataset,50000,5000,-1,'',num_p=20)\n",
    "    P.train_npe()\n",
    "    P.representation_npe()\n",
    "\n",
    "    with open('/scratch/users/briannac/tmp/ditaxa/%s/intermediate_files/npe_representation/%s_uniquepiece_-1_features' % (dataset, dataset)) as f:\n",
    "        features = [ff.replace('\\n', '').upper() for ff in f.readlines()]\n",
    "    seqs = [str(r.seq) for r in SeqIO.parse('/scratch/users/briannac/tmp/ditaxa/%s/seqs.fa' % dataset, 'fasta')]\n",
    "    asv_biomarker = scipy.sparse.csc_matrix([[1*(f in s) for f in features] for s in seqs])\n",
    "    SaveMatrices(asv_biomarker, [i for i in range(np.shape(asv_biomarker)[1])], dataset, 'ditaxa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micropheno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [00:00, 30581.51it/s]\n",
      "256it [00:00, 682607.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  256\n",
      "Computing biomarker names...\n",
      "256  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "832it [00:00, 31722.17it/s]\n",
      "3595it [00:00, 885669.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  4053\n",
      "Computing biomarker names...\n",
      "3595  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "775it [00:00, 23738.16it/s]\n",
      "9356it [00:00, 907075.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  15816\n",
      "Computing biomarker names...\n",
      "9356  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "252it [00:00, 29503.55it/s]\n",
      "256it [00:00, 668998.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  256\n",
      "Computing biomarker names...\n",
      "256  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "904it [00:00, 31059.76it/s]\n",
      "2855it [00:00, 876627.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  3962\n",
      "Computing biomarker names...\n",
      "2855  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "904it [00:00, 25664.52it/s]\n",
      "4878it [00:00, 885361.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  13350\n",
      "Computing biomarker names...\n",
      "4878  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['autism', 'obesity']:\n",
    "    for k in [4,6,8]:\n",
    "        seqs_file='/home/groups/dpwall/briannac/sequence_based_biomarkers/data/%s/seqs.fa' % dataset\n",
    "        \n",
    "        FastaRep=FastaRepresentations(seqs_file,label_modifying_func=lambda x: x.split('.')[0])\n",
    "        asv_biomarker=FastaRep.get_vector_rep(FastaRep.corpus, k,restricted=True)\n",
    "        \n",
    "        SaveMatrices(asv_biomarker, [i for i in range(np.shape(asv_biomarker)[1])], dataset, 'micropheno%s' % k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 SBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "488it [00:00, 29453.62it/s]\n",
      "644it [00:00, 868251.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  1121\n",
      "Computing biomarker names...\n",
      "644  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "498it [00:00, 28806.56it/s]\n",
      "594it [00:00, 857335.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  912\n",
      "Computing biomarker names...\n",
      "594  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['autism', 'obesity']:\n",
    "    rdp = [record for record in SeqIO.parse('/home/groups/dpwall/briannac/sequence_based_biomarkers/data/%s/seqs_aligned_rdp.fa' % dataset, 'fasta')][:-1]\n",
    "    df = pd.DataFrame([list(r.seq.upper()) for r in rdp])\n",
    "    is_interesting = df.apply(lambda x: (len(set(x))>0) & (len(set(x).intersection(['A', 'T', 'C', 'G']))>0))\n",
    "\n",
    "    df = df[df.columns[is_interesting]]\n",
    "    df_A = df=='A'\n",
    "    df_A.columns = [str(c) + '_A' for c in df_A.columns]\n",
    "    df_T = df=='T'\n",
    "    df_T.columns = [str(c) + '_T' for c in df_T.columns]\n",
    "    df_G = df=='G'\n",
    "    df_G.columns = [str(c) + '_G' for c in df_G.columns]\n",
    "    df_C = df=='C'\n",
    "    df_C.columns = [str(c) + '_C' for c in df_C.columns]\n",
    "    df_gap = df=='-'\n",
    "    df_gap.columns = [str(c) + '_gap' for c in df_gap.columns]\n",
    "    df_sbb1 = pd.concat([df_A, df_C, df_T, df_G, df_gap], axis=1)\n",
    "    df_sbb1 = df_sbb1[df_sbb1.columns[(df_sbb1.mean()>0.0) & (df_sbb1.mean()<1.0)]]\n",
    "    df_sbb1 = df_sbb1[df_sbb1.columns[~df_sbb1.transpose().duplicated().values]]\n",
    "\n",
    "    df_sbb1 = pd.concat([df_A, df_C, df_T, df_G, df_gap], axis=1)\n",
    "\n",
    "    # Get rid of biomarkers at 100% frequency, 0% frequency, and in LD w/another.\n",
    "    df_sbb1 = df_sbb1[df_sbb1.columns[(df_sbb1.mean()>0.0) & (df_sbb1.mean()<1.0)]]\n",
    "    df_sbb1 = df_sbb1[df_sbb1.columns[~df_sbb1.transpose().duplicated().values]]\n",
    "\n",
    "    # Save sparse matrix of ASV x biomarker.\n",
    "    asv_biomarker = scipy.sparse.csc_matrix(df_sbb1)\n",
    "    SaveMatrices(asv_biomarker, df_sbb1.columns, dataset, 'sbb1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1SBB small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "188it [00:00, 31183.18it/s]\n",
      "197it [00:00, 436721.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  197\n",
      "Computing biomarker names...\n",
      "197  biomarkers > 10% freq\n",
      "Writing to files...\n",
      "149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "147it [00:00, 29028.38it/s]\n",
      "149it [00:00, 410881.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  149\n",
      "Computing biomarker names...\n",
      "149  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['autism_small', 'obesity_small']:\n",
    "    rdp = [record for record in SeqIO.parse('/home/groups/dpwall/briannac/sequence_based_biomarkers/data/%s/seqs_aligned_rdp.fa' % dataset, 'fasta')][:-1]\n",
    "    df = pd.DataFrame([list(r.seq.upper()) for r in rdp])\n",
    "    is_interesting = df.apply(lambda x: (len(set(x))>0) & (len(set(x).intersection(['A', 'T', 'C', 'G']))>0))\n",
    "\n",
    "    df = df[df.columns[is_interesting]]\n",
    "    df_A = df=='A'\n",
    "    df_A.columns = [str(c) + '_A' for c in df_A.columns]\n",
    "    df_T = df=='T'\n",
    "    df_T.columns = [str(c) + '_T' for c in df_T.columns]\n",
    "    df_G = df=='G'\n",
    "    df_G.columns = [str(c) + '_G' for c in df_G.columns]\n",
    "    df_C = df=='C'\n",
    "    df_C.columns = [str(c) + '_C' for c in df_C.columns]\n",
    "    df_gap = df=='-'\n",
    "    df_gap.columns = [str(c) + '_gap' for c in df_gap.columns]\n",
    "    df_sbb1 = pd.concat([df_A, df_C, df_T, df_G, df_gap], axis=1)\n",
    "    df_sbb1 = df_sbb1[df_sbb1.columns[(df_sbb1.mean()>0.0) & (df_sbb1.mean()<1.0)]]\n",
    "    df_sbb1 = df_sbb1[df_sbb1.columns[~df_sbb1.transpose().duplicated().values]]\n",
    "\n",
    "    df_sbb1 = pd.concat([df_A, df_C, df_T, df_G, df_gap], axis=1)\n",
    "\n",
    "    # Get rid of biomarkers at 100% frequency, 0% frequency, and in LD w/another.\n",
    "    df_sbb1 = df_sbb1[df_sbb1.columns[(df_sbb1.mean()>0.0) & (df_sbb1.mean()<1.0)]]\n",
    "    df_sbb1 = df_sbb1[df_sbb1.columns[~df_sbb1.transpose().duplicated().values]]\n",
    "\n",
    "    # Save sparse matrix of ASV x biomarker.\n",
    "    asv_biomarker = scipy.sparse.csc_matrix(df_sbb1)\n",
    "    \n",
    "    asv_vs_biomarker_df = pd.DataFrame(asv_biomarker.todense())\n",
    "    asv_vs_biomarker_df.columns = df_sbb1.columns\n",
    "    df = asv_vs_biomarker_df[asv_vs_biomarker_df.columns[(asv_vs_biomarker_df.mean()>.1) & (asv_vs_biomarker_df.mean()<.9)]]\n",
    "    \n",
    "    jaccard_df = pd.DataFrame([[jaccard(df[df.columns[i]], df[df.columns[j]]) if i > j else np.nan \n",
    "                            for i in range(np.shape(df)[1])] \n",
    "                           for j in range(np.shape(df)[1])])\n",
    "    new_df = jaccard_df\n",
    "    min_val = np.min(new_df.min(axis=0))\n",
    "    min_idx = new_df.columns[np.argmin(new_df.min(axis=0))]\n",
    "    while (min_val < .1) or (min_val>.9):\n",
    "        new_df = new_df.drop(min_idx, axis=0)\n",
    "        new_df = new_df.drop(min_idx, axis=1)\n",
    "        min_val = np.min(new_df.min(axis=0))\n",
    "        min_idx = new_df.columns[np.argmin(new_df.min(axis=0))]\n",
    "    columns = df.columns[new_df.columns]\n",
    "    print(len(columns))\n",
    "    asv_biomarker = scipy.sparse.csc_matrix(df[columns])\n",
    "    SaveMatrices(asv_biomarker, columns, dataset, 'sbb1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 SBB for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5249, 57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 28505.62it/s]\n",
      "57it [00:00, 323511.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  57\n",
      "Computing biomarker names...\n",
      "34  biomarkers > 10% freq\n",
      "Writing to files...\n",
      "(12363, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:00, 27970.50it/s]\n",
      "46it [00:00, 281250.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  46\n",
      "Computing biomarker names...\n",
      "37  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['testing_autism', 'testing_obesity']:\n",
    "    rdp = [record for record in SeqIO.parse('/home/groups/dpwall/briannac/sequence_based_biomarkers/data/%s/seqs_aligned_rdp.fa' % dataset.replace('testing_', ''), 'fasta')][:-1]\n",
    "    df = pd.DataFrame([list(r.seq.upper()) for r in rdp])\n",
    "    is_interesting = df.apply(lambda x: (len(set(x))>0) & (len(set(x).intersection(['A', 'T', 'C', 'G']))>0))\n",
    "\n",
    "    df = df[df.columns[is_interesting]]\n",
    "    df_A = df=='A'\n",
    "    df_A.columns = [str(c) + '_A' for c in df_A.columns]\n",
    "    df_T = df=='T'\n",
    "    df_T.columns = [str(c) + '_T' for c in df_T.columns]\n",
    "    df_G = df=='G'\n",
    "    df_G.columns = [str(c) + '_G' for c in df_G.columns]\n",
    "    df_C = df=='C'\n",
    "    df_C.columns = [str(c) + '_C' for c in df_C.columns]\n",
    "    df_gap = df=='-'\n",
    "    df_gap.columns = [str(c) + '_gap' for c in df_gap.columns]\n",
    "    df_sbb1 = pd.concat([df_A, df_C, df_T, df_G, df_gap], axis=1)\n",
    "    df_sbb1 = df_sbb1[df_sbb1.columns[(df_sbb1.mean()>0.0) & (df_sbb1.mean()<1.0)]]\n",
    "    df_sbb1 = df_sbb1[df_sbb1.columns[~df_sbb1.transpose().duplicated().values]]\n",
    "    df_sbb1 = pd.concat([df_A, df_C, df_T, df_G, df_gap], axis=1)\n",
    "\n",
    "    # Get rid of biomarkers at 100% frequency, 0% frequency, and in LD w/another.\n",
    "    df_sbb1 = df_sbb1[df_sbb1.columns[(df_sbb1.mean()>0.0) & (df_sbb1.mean()<1.0)]]\n",
    "    df_sbb1 = df_sbb1[df_sbb1.columns[~df_sbb1.transpose().duplicated().values]]\n",
    "    df_sbb1 = df_sbb1[df_sbb1.columns[::20]] # JUST 100 COLUMNS FOR TESTING\n",
    "    print(np.shape(df_sbb1))\n",
    "    # Save sparse matrix of ASV x biomarker.\n",
    "    asv_biomarker = scipy.sparse.csc_matrix(df_sbb1)\n",
    "    SaveMatrices(asv_biomarker, df_sbb1.columns, dataset, 'sbb1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [00:00, 30300.47it/s]\n",
      "181it [00:00, 423643.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  506\n",
      "Computing biomarker names...\n",
      "181  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [00:00, 26982.66it/s]\n",
      "297it [00:00, 464469.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  1334\n",
      "Computing biomarker names...\n",
      "297  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:00, 23114.90it/s]\n",
      "365it [00:00, 473090.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  1977\n",
      "Computing biomarker names...\n",
      "365  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:00, 18932.86it/s]\n",
      "420it [00:00, 483825.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  2829\n",
      "Computing biomarker names...\n",
      "420  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 31492.07it/s]\n",
      "156it [00:00, 420508.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  689\n",
      "Computing biomarker names...\n",
      "156  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [00:00, 28306.74it/s]\n",
      "218it [00:00, 447118.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  2077\n",
      "Computing biomarker names...\n",
      "218  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 22047.43it/s]\n",
      "283it [00:00, 462943.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  3809\n",
      "Computing biomarker names...\n",
      "283  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:00, 3657.22it/s]\n",
      "288it [00:00, 458951.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  9525\n",
      "Computing biomarker names...\n",
      "288  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['autism', 'obesity']:\n",
    "    for similarity in [90, 95, 97, 99]:\n",
    "        otu_mapping = pd.read_table('/home/groups/dpwall/briannac/sequence_based_biomarkers/results/generate_biomarkers/otu_asv_map_%i_%s.txt' % (similarity, dataset), header=None)\n",
    "        otu_dict = {otu:idx for idx, otu in enumerate(np.unique(otu_mapping[1]))}\n",
    "        otu_mapping[1] = [otu_dict[i] for i in otu_mapping[1]]\n",
    "        mat = np.zeros((len(otu_mapping), len(otu_dict)))\n",
    "        for i, otu_id in enumerate(otu_mapping[1]):\n",
    "            mat[i][otu_id] = 1\n",
    "        asv_biomarker = scipy.sparse.csc_matrix(mat)\n",
    "        SaveMatrices(asv_biomarker, list(otu_dict.keys()), dataset, 'otu%s' % similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 964.65it/s]\n",
      "416it [00:00, 482530.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  5249\n",
      "Computing biomarker names...\n",
      "416  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 474.47it/s]\n",
      "278it [00:00, 461788.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  12363\n",
      "Computing biomarker names...\n",
      "278  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['autism', 'obesity']:\n",
    "    sample_vs_asv = pd.read_table('/home/groups/dpwall/briannac/sequence_based_biomarkers/data/%s/sample_vs_asv.tsv' % dataset, index_col=0)\n",
    "\n",
    "    # Save sparse matrix of ASV x biomarker.\n",
    "    asv_biomarker = scipy.sparse.csc_matrix(np.identity(len(sample_vs_asv)))\n",
    "    SaveMatrices(asv_biomarker, sample_vs_asv.index, dataset, 'asv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [00:00, 30719.79it/s]\n",
      "183it [00:00, 670356.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  358\n",
      "Computing biomarker names...\n",
      "183  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 30964.90it/s]\n",
      "101it [00:00, 541027.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biomarkers after LD filtering:  192\n",
      "Computing biomarker names...\n",
      "101  biomarkers > 10% freq\n",
      "Writing to files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['autism', 'obesity']:\n",
    "    annotations = pd.read_table('/home/groups/dpwall/briannac/sequence_based_biomarkers/data/%s/asv_vs_taxa_annotation.tsv' % dataset, index_col=0)\n",
    "    sample_vs_asv = pd.read_table('/home/groups/dpwall/briannac/sequence_based_biomarkers/data/%s/sample_vs_asv.tsv' % dataset, index_col=0)\n",
    "    annotations['Kingdom'] = ['k_' +a for a in annotations['Kingdom']]\n",
    "    annotations['Phylum'] = ['p_' +a for a in annotations['Phylum']]\n",
    "    annotations['Class'] = ['c_' +a for a in annotations['Class']]\n",
    "    annotations['Order'] = ['o_' +a for a in annotations['Order']]\n",
    "    annotations['Family'] = ['f_' +a for a in annotations['Family']]\n",
    "    annotations['Genus'] = ['g_' +a for a in annotations['Genus']]\n",
    "    taxa = np.unique(np.concatenate(annotations.values))\n",
    "    taxa_df = pd.DataFrame(np.zeros((len(annotations), len(taxa))))\n",
    "    taxa_df.columns = taxa\n",
    "    taxa_df.index = annotations.index\n",
    "    taxa_df = taxa_df[[c for c in taxa_df.columns if 'unclassified' not in c]]\n",
    "    for row in annotations.iterrows():\n",
    "        taxa_df.loc[row[0],list([r for r in row[1].values if 'unclassified' not in r])]  = 1\n",
    "        \n",
    "        \n",
    "    # Save sparse matrix of ASV x biomarker.\n",
    "    asv_biomarker = scipy.sparse.csc_matrix(taxa_df)\n",
    "    SaveMatrices(asv_biomarker, taxa_df.columns, dataset, 'taxa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36_env",
   "language": "python",
   "name": "py_36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
